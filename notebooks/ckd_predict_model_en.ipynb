{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc29c90fa8fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note:\n",
    "The dataset used in this study contains clinical data and is not publicly available due to privacy constraints.\n",
    "All code in this notebook is provided for demonstration and reproducibility purposes.\n",
    "\n",
    "Author: Jiadong Xie\n",
    "Date: 2025-06-07\n",
    "Project: early-stage CKD Prediction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee641c0132787e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79add57b7eaeb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a94a7c4883145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac8ad30f5e72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 Feature Definition and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b10ddb62e6cab0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "continuous_features = []\n",
    "categorical_features = []\n",
    "features = continuous_features + categorical_features\n",
    "processed_data_file = \"\"\n",
    "processed_labels_file = \"\"\n",
    "X = pd.read_csv(processed_data_file)\n",
    "y = pd.read_csv(processed_labels_file)['CKD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c44d14504217a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 (Optional) Feature Name Conversion (Chinese to English)\n",
    "# This step can be skipped if the original dataset already uses English feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fbc875fd7e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping = {}\n",
    "xueChangGui_all = []\n",
    "niaoChangGui_all = []\n",
    "xueShengHua_all = []\n",
    "selected_features =  xueChangGui_all + niaoChangGui_all  + xueShengHua_all + ['SEX', 'AGE']\n",
    "selected_features_en = [feature_mapping[feature] for feature in selected_features]\n",
    "X.columns = selected_features_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2add6ee7edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab265a168bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Number of training samples: {len(y_train)}\")\n",
    "print(f\"Number of validation samples: {len(y_valid)}\")\n",
    "print(f\"Number of test samples: {len(y_test)}\")\n",
    "\n",
    "print(\"Number of positive samples in training set:\", np.sum(y_train == 1))\n",
    "print(\"Number of negative samples in training set:\", np.sum(y_train == 0))\n",
    "print(\"Number of positive samples in validation set:\", np.sum(y_valid == 1))\n",
    "print(\"Number of negative samples in validation set:\", np.sum(y_valid == 0))\n",
    "print(\"Number of positive samples in test set:\", np.sum(y_test == 1))\n",
    "print(\"Number of negative samples in test set:\", np.sum(y_test == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449114c32e19c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970152344656c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Definition of Models and Their Parameters (For hyperparameter tuning, see Section 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368de799ee2aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Decision Tree':DecisionTreeClassifier(),\n",
    "    'Random Forest':RandomForestClassifier(),\n",
    "    'MLP': MLPClassifier(),\n",
    "    'AdaBoost':AdaBoostClassifier(),\n",
    "    'XGBoost':xgb.XGBClassifier(),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bcbd5cdc7a5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Model Training and Related Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ea8d022aaaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import os\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "output_dir = 'pic_train'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_scores = {}\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', model_name='', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, xticklabels=classes, yticklabels=classes, cbar=True, annot_kws={\"size\": 20})\n",
    "    plt.title(f'{title} ({model_name})', fontsize=20)\n",
    "    plt.ylabel('True Label', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=16)\n",
    "    plt.savefig(f'{output_dir}/confusion_matrix_{model_name}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(models_fpr_tpr_auc, title='ROC Curve'):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    colors = plt.cm.tab10.colors \n",
    "    for i, (name, (fpr, tpr, auc_score)) in enumerate(models_fpr_tpr_auc.items()):\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', linewidth=2, color=colors[i])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--', linewidth=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.legend(loc='lower right', fontsize=16)\n",
    "    plt.savefig(f'{output_dir}/roc_curve.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prc_curve(models_prc, title='Precision-Recall Curve'):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    colors = plt.cm.tab10.colors\n",
    "    for i, (name, (precision, recall, pr_auc)) in enumerate(models_prc.items()):\n",
    "        plt.plot(recall, precision, label=f'{name} (PR-AUC = {pr_auc:.4f})', linewidth=2, color=colors[i])\n",
    "    plt.xlabel('Recall', fontsize=16)\n",
    "    plt.ylabel('Precision', fontsize=16)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.legend(loc='lower left', fontsize=16)\n",
    "    plt.savefig(f'{output_dir}/prc_curve.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "models_fpr_tpr_auc = {} \n",
    "models_prc = {}\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    if not hasattr(model, 'predict_proba'):\n",
    "        print(f\"Model {name} does not support `predict_proba`, skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Training and evaluating model: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_prob)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    models_fpr_tpr_auc[name] = (fpr, tpr, auc_score)\n",
    "    precision, recall, _ = precision_recall_curve(y_valid, y_prob)\n",
    "    pr_auc = average_precision_score(y_valid, y_prob)\n",
    "    models_prc[name] = (precision, recall, pr_auc)\n",
    "    y_pred_best = (y_prob >= thresholds[np.argmax(tpr - fpr)]).astype(int)\n",
    "    cm = confusion_matrix(y_valid, y_pred_best)\n",
    "    plot_confusion_matrix(cm, classes=[\"Negative\", \"Positive\"], model_name=name)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    NPV=tn / (tn + fn) \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    model_scores[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV\":precision,\n",
    "        \"NPV\":NPV,\n",
    "        \"AUC\": auc_score,\n",
    "        \"PR-AUC\":pr_auc\n",
    "    }\n",
    "plot_roc_curve(models_fpr_tpr_auc, title='ROC Curve Comparison')\n",
    "plot_prc_curve(models_prc, title='Precision-Recall Curve Comparison')\n",
    "df_model_scores= pd.DataFrame.from_dict(model_scores, orient='index')\n",
    "df_model_scores.to_csv(\"df_model_scores.csv\", index=True)\n",
    "df_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9330496f64d2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Different Feature Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41e04ee4fe5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_score, recall_score, f1_score,\n",
    "    accuracy_score, confusion_matrix, average_precision_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "output_dir = \"combination\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "models_fpr_tpr_auc = {}\n",
    "models_prc = {}\n",
    "\n",
    "def plot_and_save_confusion_matrix(cm, feature_set_name):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True, annot_kws={\"size\": 14})\n",
    "    plt.title(f'Confusion Matrix - {feature_set_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    cm_path = os.path.join(output_dir, f'confusion_matrix_{feature_set_name}.png')\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "xueChangGui_all = []\n",
    "niaoChangGui_all = []\n",
    "xueShengHua_all = []\n",
    "basci_info = []\n",
    "feature_sets = {\n",
    "    'XCG+Basic': xueChangGui_all + basci_info,\n",
    "    'NCG+Basic': niaoChangGui_all + basci_info,\n",
    "    'XSH+Basic': xueShengHua_all + basci_info,\n",
    "    'XCG+NCG+Basic': xueChangGui_all + niaoChangGui_all + basci_info,\n",
    "    'XCG+XSH+Basic': xueChangGui_all + xueShengHua_all + basci_info,\n",
    "    'NCG+XSH+Basic': niaoChangGui_all + xueShengHua_all + basci_info,\n",
    "    'XCG+NCG+XSH+Basic': xueChangGui_all + niaoChangGui_all + xueShengHua_all + basci_info\n",
    "}\n",
    "feature_combination_results = {}\n",
    "for feature_set_name, feature_list in feature_sets.items():\n",
    "    print(f\"\\nEvaluating feature combination: {feature_set_name}\")\n",
    "    X_train_subset = X_train[feature_list]\n",
    "    X_valid_subset = X_valid[feature_list]\n",
    "    \n",
    "    model = xgb.XGBClassifier() # Use the optimized hyperparameters in this section\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    y_prob = model.predict_proba(X_valid_subset)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, y_prob)\n",
    "    auc_score = roc_auc_score(y_valid, y_prob)\n",
    "    precision, recall, _ = precision_recall_curve(y_valid, y_prob)\n",
    "    pr_auc = average_precision_score(y_valid, y_prob)\n",
    "    models_fpr_tpr_auc[feature_set_name] = (fpr, tpr, auc_score)\n",
    "    models_prc[feature_set_name] = (precision, recall, pr_auc)\n",
    "    youden_index = tpr - fpr\n",
    "    best_threshold = thresholds[np.argmax(youden_index)]\n",
    "    best_youden = np.max(youden_index)\n",
    "    y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "    cm = confusion_matrix(y_valid, y_pred_best)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = accuracy_score(y_valid, y_pred_best)\n",
    "    recall = recall_score(y_valid, y_pred_best)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = precision_score(y_valid, y_pred_best)\n",
    "    npv = tn / (tn + fn)\n",
    "    f1 = f1_score(y_valid, y_pred_best)\n",
    "    feature_combination_results[feature_set_name] = {\n",
    "        \"Youden’s J\": best_youden,\n",
    "        \"Optimal Threshold\": best_threshold,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV\": ppv,\n",
    "        \"NPV\": npv,\n",
    "        \"AUC\": auc_score,\n",
    "        \"PR-AUC\": pr_auc\n",
    "    }\n",
    "    plot_and_save_confusion_matrix(cm, feature_set_name)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = plt.cm.tab10.colors\n",
    "for i, (name, (fpr, tpr, auc_score)) in enumerate(models_fpr_tpr_auc.items()):\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', linewidth=2, color=colors[i])\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", linewidth=2)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "roc_path = os.path.join(output_dir, \"roc_curve_all.png\")\n",
    "plt.savefig(roc_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, (name, (precision, recall, pr_auc)) in enumerate(models_prc.items()):\n",
    "    plt.plot(recall, precision, label=f'{name} (PR-AUC = {pr_auc:.4f})', linewidth=2, color=colors[i])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve Comparison\")\n",
    "plt.legend()\n",
    "prc_path = os.path.join(output_dir, \"prc_curve_all.png\")\n",
    "plt.savefig(prc_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "df_feature_results = pd.DataFrame.from_dict(feature_combination_results, orient='index')\n",
    "df_feature_results.to_csv(\"feature_combination_performance.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c281a1bdfd13c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Internal Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d55eb74aa53a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_score, recall_score, f1_score,\n",
    "    accuracy_score, confusion_matrix, average_precision_score\n",
    ")\n",
    "feature_list = feature_sets['XCG+NCG+Basic']\n",
    "X_train_subset = X_train[feature_list]\n",
    "X_valid_subset = X_valid[feature_list]\n",
    "X_test_subset = X_test[feature_list]\n",
    "model = xgb.XGBClassifier() # Use the optimized hyperparameters in this section\n",
    "model.fit(X_train_subset, y_train)\n",
    "y_prob = model.predict_proba(X_test_subset)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "youden_index = tpr - fpr\n",
    "best_threshold = thresholds[np.argmax(youden_index)]\n",
    "best_youden = np.max(youden_index)\n",
    "y_pred_best = (y_prob >= best_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_best).ravel()\n",
    "accuracy = accuracy_score(y_test, y_pred_best)\n",
    "precision = precision_score(y_test, y_pred_best)\n",
    "recall = recall_score(y_test, y_pred_best)\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = precision \n",
    "npv = tn / (tn + fn)\n",
    "f1 = f1_score(y_test, y_pred_best)\n",
    "pr_auc = average_precision_score(y_test, y_prob)\n",
    "\n",
    "evaluation_results = {\n",
    "    \"AUC\": auc_score,\n",
    "    \"Youden’s J\": best_youden,\n",
    "    \"Optimal Threshold\": best_threshold,\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision\": precision,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Sensitivity\": recall,\n",
    "    \"Specificity\": specificity,\n",
    "    \"PPV\": ppv,\n",
    "    \"NPV\": npv,\n",
    "    \"PR-AUC\": pr_auc\n",
    "}\n",
    "df_evaluation = pd.DataFrame([evaluation_results], index=['XCG+NCG+Basic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e72d4dd9c0cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model.pkl\"\n",
    "joblib.dump(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83366950cb30a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Interpretability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b2ead37e95677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section uses SHAP. Below is an example code for generating bar plots.\n",
    "# For more details, please refer to: https://shap.readthedocs.io/en/latest/api_examples.html#plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe570eeff0f511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "explainer = shap.Explainer(model, data_shap)\n",
    "shap_values = explainer(data_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1b6134ac05a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "shap.plots.bar(shap_values, max_display=20, show=False)\n",
    "shap_bar_path = \"SHAP_Feature_Importance_Bar_TestSet.png\"\n",
    "plt.savefig(shap_bar_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810276bd6ef9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Other Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cc1442a9d97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 DeLong Test Function (Example Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dee30f065fe40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delong_test(y_true, prob1, prob2):\n",
    "    auc1 = roc_auc_score(y_true, prob1)\n",
    "    auc2 = roc_auc_score(y_true, prob2)\n",
    "    n1 = np.sum(y_true == 1) \n",
    "    n2 = np.sum(y_true == 0) \n",
    "    q1 = auc1 * (1 - auc1)\n",
    "    q2 = auc2 * (1 - auc2)\n",
    "    var = (q1 / n1) + (q2 / n2)\n",
    "    z = (auc1 - auc2) / np.sqrt(var)\n",
    "    p_value = 2 * norm.sf(abs(z))\n",
    "    return auc1, auc2, z, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c036591dec6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Cross-Validation (Example Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7474d3e564d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "xgb = XGBClassifier()\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
